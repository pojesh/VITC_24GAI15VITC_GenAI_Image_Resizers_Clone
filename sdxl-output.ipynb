{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12861965,"sourceType":"datasetVersion","datasetId":8135527},{"sourceId":12862123,"sourceType":"datasetVersion","datasetId":8135644}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Outpainting inference notebook**\n\n- **Source** : https://www.kaggle.com/code/pojesh/sdxl-output","metadata":{}},{"cell_type":"code","source":"%pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''import shutil\n\nsrc_path = r'/kaggle/input/mysdxlcomps/pipeline_fill_sd_xl.py'\ndst_path = r'/kaggle/working/''''\n\n'''shutil.copy(src_path, dst_path)\nprint('Copied')'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom PIL import Image\nimport numpy as np\nfrom diffusers import AutoencoderKL, TCDScheduler\nfrom diffusers.models.model_loading_utils import load_state_dict\nfrom huggingface_hub import hf_hub_download\nimport logging\n\n# modules (keep them in the same folder)\nfrom controlnet_union import ControlNetModel_Union\nfrom pipeline_fill_sd_xl import StableDiffusionXLFillPipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:42:21.855413Z","iopub.execute_input":"2025-08-25T13:42:21.855695Z","iopub.status.idle":"2025-08-25T13:42:49.078581Z","shell.execute_reply.started":"2025-08-25T13:42:21.855672Z","shell.execute_reply":"2025-08-25T13:42:49.077981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import ImageDraw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:42:49.079458Z","iopub.execute_input":"2025-08-25T13:42:49.080063Z","iopub.status.idle":"2025-08-25T13:42:49.083570Z","shell.execute_reply.started":"2025-08-25T13:42:49.080040Z","shell.execute_reply":"2025-08-25T13:42:49.082837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del pipe","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del vae\ndel model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# CONFIGURATION – edit only these two lines if you want other sizes\n# ------------------------------------------------------------------\nTARGET_WIDTH  = 1800         # << hard-coded width\nTARGET_HEIGHT = 1200         # << hard-coded height\nINPUT_IMAGE   = r\"/kaggle/input/flux-images-set1/robot_1200x1200.webp\"  # source file \nOUTPUT_DIR    = r\"/kaggle/working/outputs\"\nOUTPUT_NAME   = \"robot_op_1800x1200.webp\"\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:42:55.712902Z","iopub.execute_input":"2025-08-25T13:42:55.713169Z","iopub.status.idle":"2025-08-25T13:42:55.717512Z","shell.execute_reply.started":"2025-08-25T13:42:55.713150Z","shell.execute_reply":"2025-08-25T13:42:55.716742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ------------------------------------------------------------------\n# Model loading \n# ------------------------------------------------------------------\nlogger.info(\"Loading models…\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype  = torch.float16 if device == \"cuda\" else torch.float32\n\n# ControlNet\nconfig_file = hf_hub_download(\n    \"xinsir/controlnet-union-sdxl-1.0\",\n    filename=\"config_promax.json\",\n)\nconfig = ControlNetModel_Union.load_config(config_file)\ncontrolnet_model = ControlNetModel_Union.from_config(config)\n\nmodel_file = hf_hub_download(\n    \"xinsir/controlnet-union-sdxl-1.0\",\n    filename=\"diffusion_pytorch_model_promax.safetensors\",\n)\nstate_dict = load_state_dict(model_file)\nloaded_keys = list(state_dict.keys())\n\nmodel = ControlNetModel_Union._load_pretrained_model(\n    controlnet_model, state_dict, model_file,\n    \"xinsir/controlnet-union-sdxl-1.0\", loaded_keys\n)[0]\nmodel = model.to(device=device, dtype=dtype)\n\n# VAE & pipeline\nvae = AutoencoderKL.from_pretrained(\n    \"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=dtype\n).to(device)\n\npipe = StableDiffusionXLFillPipeline.from_pretrained(\n    \"SG161222/RealVisXL_V5.0_Lightning\",\n    torch_dtype=dtype,\n    vae=vae,\n    controlnet=model,\n    variant=\"fp16\" if dtype == torch.float16 else None,\n).to(device)\n\npipe.scheduler = TCDScheduler.from_config(pipe.scheduler.config)\nlogger.info(\"Models loaded.\")\n\n\n\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef prepare_image_and_mask(image, width, height, overlap_percentage=10,\n                           resize_option=\"Full\", custom_resize_percentage=50,\n                           alignment=\"Middle\", overlap_left=True,\n                           overlap_right=True, overlap_top=True,\n                           overlap_bottom=True):\n    target_size = (width, height)\n    scale_factor = min(target_size[0] / image.width, target_size[1] / image.height)\n    new_width = int(image.width * scale_factor)\n    new_height = int(image.height * scale_factor)\n    source = image.resize((new_width, new_height), Image.LANCZOS)\n\n    if resize_option == \"Full\":\n        resize_percentage = 100\n    elif resize_option == \"50%\":\n        resize_percentage = 50\n    elif resize_option == \"33%\":\n        resize_percentage = 33\n    elif resize_option == \"25%\":\n        resize_percentage = 25\n    else:\n        resize_percentage = custom_resize_percentage\n\n    resize_factor = resize_percentage / 100\n    new_width = max(int(source.width * resize_factor), 64)\n    new_height = max(int(source.height * resize_factor), 64)\n    source = source.resize((new_width, new_height), Image.LANCZOS)\n\n    overlap_x = max(int(new_width * (overlap_percentage / 100)), 1)\n    overlap_y = max(int(new_height * (overlap_percentage / 100)), 1)\n\n    if alignment == \"Middle\":\n        margin_x = (target_size[0] - new_width) // 2\n        margin_y = (target_size[1] - new_height) // 2\n    elif alignment == \"Left\":\n        margin_x, margin_y = 0, (target_size[1] - new_height) // 2\n    elif alignment == \"Right\":\n        margin_x, margin_y = target_size[0] - new_width, (target_size[1] - new_height) // 2\n    elif alignment == \"Top\":\n        margin_x, margin_y = (target_size[0] - new_width) // 2, 0\n    elif alignment == \"Bottom\":\n        margin_x, margin_y = (target_size[0] - new_width) // 2, target_size[1] - new_height\n\n    margin_x = max(0, min(margin_x, target_size[0] - new_width))\n    margin_y = max(0, min(margin_y, target_size[1] - new_height))\n\n    background = Image.new('RGB', target_size, (255, 255, 255))\n    background.paste(source, (margin_x, margin_y))\n\n    mask = Image.new('L', target_size, 255)\n    mask_draw = ImageDraw.Draw(mask)\n\n    white_gaps_patch = 2\n    left_overlap   = margin_x + overlap_x if overlap_left   else margin_x + white_gaps_patch\n    right_overlap  = margin_x + new_width - overlap_x if overlap_right  else margin_x + new_width - white_gaps_patch\n    top_overlap    = margin_y + overlap_y if overlap_top    else margin_y + white_gaps_patch\n    bottom_overlap = margin_y + new_height - overlap_y if overlap_bottom else margin_y + new_height - white_gaps_patch\n\n    if alignment == \"Left\":\n        left_overlap = margin_x + overlap_x if overlap_left else margin_x\n    elif alignment == \"Right\":\n        right_overlap = margin_x + new_width - overlap_x if overlap_right else margin_x + new_width\n    elif alignment == \"Top\":\n        top_overlap = margin_y + overlap_y if overlap_top else margin_y\n    elif alignment == \"Bottom\":\n        bottom_overlap = margin_y + new_height - overlap_y if overlap_bottom else margin_y + new_height\n\n    mask_draw.rectangle([(left_overlap, top_overlap),\n                         (right_overlap, bottom_overlap)], fill=0)\n    return background, mask\n\ndef can_expand(source_width, source_height, target_width, target_height, alignment):\n    if alignment in (\"Left\", \"Right\") and source_width >= target_width:\n        return False\n    if alignment in (\"Top\", \"Bottom\") and source_height >= target_height:\n        return False\n    return True\n\ndef process_outpaint(image, width, height, num_inference_steps=8, prompt_input=\"\"):\n    overlap_percentage = 10\n    resize_option = \"Full\"\n    custom_resize_percentage = 50\n    alignment = \"Middle\"\n    overlap_left = overlap_right = overlap_top = overlap_bottom = True\n\n    background, mask = prepare_image_and_mask(\n        image, width, height, overlap_percentage, resize_option,\n        custom_resize_percentage, alignment, overlap_left, overlap_right,\n        overlap_top, overlap_bottom\n    )\n\n    if not can_expand(background.width, background.height, width, height, alignment):\n        alignment = \"Middle\"\n\n    cnet_image = background.copy()\n    cnet_image.paste(0, (0, 0), mask)\n\n    final_prompt = f\"{prompt_input} , high quality, 4k\" if prompt_input else \"high quality, 4k\"\n\n    with torch.no_grad(), torch.autocast(device_type=device, dtype=dtype):\n        (prompt_embeds, negative_prompt_embeds,\n         pooled_prompt_embeds, negative_pooled_prompt_embeds) = pipe.encode_prompt(final_prompt, device, True)\n\n        result_image = None\n        for img in pipe(prompt_embeds=prompt_embeds,\n                        negative_prompt_embeds=negative_prompt_embeds,\n                        pooled_prompt_embeds=pooled_prompt_embeds,\n                        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n                        image=cnet_image,\n                        num_inference_steps=num_inference_steps):\n            result_image = img\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    result_image = result_image.convert(\"RGBA\")\n    cnet_image.paste(result_image, (0, 0), mask)\n    return cnet_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:43:00.280470Z","iopub.execute_input":"2025-08-25T13:43:00.280734Z","iopub.status.idle":"2025-08-25T13:44:26.724868Z","shell.execute_reply.started":"2025-08-25T13:43:00.280715Z","shell.execute_reply":"2025-08-25T13:44:26.724044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# Main execution\n# ------------------------------------------------------------------\nif __name__ == \"__main__\":\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    if not os.path.isfile(INPUT_IMAGE):\n        logger.error(f\"Input file '{INPUT_IMAGE}' not found.\")\n        exit(1)\n\n    with Image.open(INPUT_IMAGE) as im:\n        if im.mode != \"RGB\":\n            im = im.convert(\"RGB\")\n\n    logger.info(f\"Outpainting {INPUT_IMAGE} -> {TARGET_WIDTH}x{TARGET_HEIGHT}\")\n    result = process_outpaint(im, TARGET_WIDTH, TARGET_HEIGHT)\n    out_path = os.path.join(OUTPUT_DIR, OUTPUT_NAME)\n    result.save(out_path)\n    logger.info(f\"Saved: {out_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T13:44:43.794481Z","iopub.execute_input":"2025-08-25T13:44:43.795234Z","iopub.status.idle":"2025-08-25T13:45:28.200886Z","shell.execute_reply.started":"2025-08-25T13:44:43.795210Z","shell.execute_reply":"2025-08-25T13:45:28.200302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}